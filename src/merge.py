# -*- coding: utf-8 -*-

import sys
from pathlib import Path
from datetime import datetime
import argparse
import json
import logging
import copy

import config
from core.yaml_handler import (
    load_yaml_file, save_yaml_file,
    FlowStyleDict, SingleQuotedString
)
from core.logger import setup_logger
from core import csvtool
from core import filters
from core import proxy_tools
from core import source_manager
from core import geoip

setup_logger(name=None)
logger = logging.getLogger("Merge")


def check_content_changes(new_proxies: list, output_path: Path, manual_file_path: Path) -> bool:
    """
    æ£€æŸ¥æ–°ç”Ÿæˆçš„ä»£ç†åˆ—è¡¨ä¸ç°æœ‰æ–‡ä»¶æ˜¯å¦ä¸€è‡´ã€‚
    æŒ‡çº¹ä»…åŸºäº (server, port)ï¼Œå¿½ç•¥åç§°ã€å¯†ç ç­‰å…¶ä»–å­—æ®µã€‚
    å¯¹æ¯”é€»è¾‘ï¼š(æ–°æŠ“å–èŠ‚ç‚¹) vs (æ—§æ–‡ä»¶ä¸­çš„è‡ªåŠ¨èŠ‚ç‚¹)ã€‚
    æ—§æ–‡ä»¶ä¸­çš„è‡ªåŠ¨èŠ‚ç‚¹é€šè¿‡æ’é™¤æ‰‹åŠ¨èŠ‚ç‚¹(åç§°å«'|M|')å’Œæ—¶é—´æˆ³èŠ‚ç‚¹æ¥è¯†åˆ«ã€‚
    åœ¨å¯¹æ¯”å‰ï¼Œä¼šä»æ–°æŠ“å–èŠ‚ç‚¹ä¸­æ’é™¤ä¸æ‰‹åŠ¨èŠ‚ç‚¹IPå†²çªçš„èŠ‚ç‚¹ã€‚
    è¿”å› True è¡¨ç¤ºæœ‰å˜åŒ–ï¼ˆæˆ–æ— æ—§æ–‡ä»¶ï¼‰ï¼Œéœ€è¦æ›´æ–°ï¼›False è¡¨ç¤ºæ— å˜åŒ–ã€‚
    """
    if not output_path.is_file():
        logger.info("æœªæ‰¾åˆ°ç°æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
        return True

    logger.info("æ£€æŸ¥è‡ªåŠ¨æŠ“å–èŠ‚ç‚¹ä¸ç°æœ‰é…ç½®æ–‡ä»¶çš„å·®å¼‚")
    try:
        # 1. åŠ è½½æ—§æ–‡ä»¶
        logger.info("ä»mergeä¸­åŠ è½½ä¸Šä¸€æ¬¡çš„èŠ‚ç‚¹")
        old_config = load_yaml_file(output_path, exit_on_error=False)
        old_proxies = old_config.get('proxies', []) if old_config else []

        # 2. åŠ è½½æ‰‹åŠ¨èŠ‚ç‚¹ï¼Œä»¥æ’é™¤IPå†²çª
        logger.info("ä»è‡ªåŠ¨æŠ“å–çš„èŠ‚ç‚¹ä¸­ç§»é™¤æ‰‹å·¥èŠ‚ç‚¹")
        manual_proxies = []
        if manual_file_path.is_file():
            manual_data = load_yaml_file(manual_file_path, exit_on_error=False)
            if manual_data and isinstance(manual_data, dict):
                manual_proxies = manual_data.get('proxies', [])
        manual_servers = {p.get('server') for p in manual_proxies if p.get('server')}

        # å®šä¹‰ç®€åŒ–æŒ‡çº¹: (server, port)
        def get_fingerprint(p):
            # å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥é¿å…ç±»å‹ä¸åŒ¹é… (å¦‚ 443 vs "443", str vs SingleQuotedString)
            return (str(p.get('server')), str(p.get('port')))

        # 3. æ„å»ºæ–°æŠ“å–èŠ‚ç‚¹çš„æŒ‡çº¹é›†åˆ (æ’é™¤ä¸æ‰‹åŠ¨èŠ‚ç‚¹IPå†²çªçš„èŠ‚ç‚¹)
        # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†ç¡®ä¿ä¸åç»­ merge_manual_nodes çš„è¡Œä¸ºä¸€è‡´
        new_set = {
            get_fingerprint(p)
            for p in new_proxies
            if p.get('server') and p.get('server') not in manual_servers
        }

        # 4. æ„å»ºæ—§æ–‡ä»¶ä¸­è‡ªåŠ¨æŠ“å–èŠ‚ç‚¹çš„æŒ‡çº¹é›†åˆ
        # ä»æ—§æ–‡ä»¶ä¸­ç§»é™¤æ‰‹åŠ¨èŠ‚ç‚¹ï¼ˆå«'|M|'ï¼‰å’Œæ—¶é—´æˆ³èŠ‚ç‚¹
        current_set = {
            get_fingerprint(p)
            for p in old_proxies
            if p.get('server') and
               '|M|' not in p.get('name', '') and
               '-Timestamp' not in p.get('name', '')
        }

        # 5. é›†åˆå¯¹æ¯”
        if new_set == current_set:
            logger.info("è‡ªåŠ¨æŠ“å–çš„ä»£ç†åˆ—è¡¨(IP/Port)ä¸ç°æœ‰æ–‡ä»¶ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return False

        # æ‰¾å‡ºå¹¶è®°å½•å·®å¼‚
        added_proxies = new_set - current_set
        removed_proxies = current_set - new_set

        logger.info("ä»£ç†åˆ—è¡¨æœ‰æ›´æ–°ï¼Œå°†ç»§ç»­ç”Ÿæˆæ–°æ–‡ä»¶ã€‚")
        if added_proxies:
            logger.info(f"  - æ–°å¢èŠ‚ç‚¹ ({len(added_proxies)}):")
            for ip, port in sorted(list(added_proxies)):
                logger.info(f"    - {ip}:{port}")
        if removed_proxies:
            logger.info(f"  - ç§»é™¤èŠ‚ç‚¹ ({len(removed_proxies)}):")
            for ip, port in sorted(list(removed_proxies)):
                logger.info(f"    - {ip}:{port}")

        return True
    except Exception as e:
        logger.warning(f"å·®å¼‚æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}ï¼Œå°†é»˜è®¤è§†ä¸ºæœ‰æ›´æ–°ã€‚")
        return True


def merge_manual_nodes(unique_proxies: list, manual_file_path: Path) -> list:
    """åŠ è½½å¹¶åˆå¹¶æ‰‹åŠ¨é…ç½®çš„èŠ‚ç‚¹ï¼ŒåŒæ—¶ç§»é™¤ä¸æ‰‹åŠ¨èŠ‚ç‚¹IPé‡å¤çš„è‡ªåŠ¨æŠ“å–èŠ‚ç‚¹ã€‚"""
    logger.info("æ­£åœ¨åŠ è½½å¹¶æ·»åŠ æ‰‹åŠ¨é…ç½®èŠ‚ç‚¹")
    if not manual_file_path.is_file():
        return unique_proxies

    try:
        manual_data = load_yaml_file(manual_file_path, exit_on_error=False)
        if manual_data and isinstance(manual_data, dict):
            manual_proxies = manual_data.get('proxies', [])

            # æå–æ‰‹åŠ¨é…ç½®ä¸­çš„æœåŠ¡å™¨åœ°å€
            manual_servers = {p.get('server') for p in manual_proxies if isinstance(p, dict) and p.get('server')}
            
            if manual_servers:
                # æ‰¾å‡ºå°†è¦è¢«ç§»é™¤çš„èŠ‚ç‚¹ä»¥ä¾¿è®°å½•æ—¥å¿—
                removed_proxies = [p for p in unique_proxies if p.get('server') in manual_servers]
                if removed_proxies:
                    unique_proxies = [p for p in unique_proxies if p.get('server') not in manual_servers]
                    removed_servers = [str(p.get('server')) for p in removed_proxies]
                    logger.info(f"å·²ç§»é™¤ {len(removed_proxies)} ä¸ªä¸æ‰‹åŠ¨é…ç½®é‡å¤çš„è‡ªåŠ¨æŠ“å–èŠ‚ç‚¹: {', '.join(removed_servers)}")

            for proxy in manual_proxies:
                if isinstance(proxy, dict):
                    ordered_proxy = proxy_tools.reorder_proxy_keys(proxy)
                    unique_proxies.append(FlowStyleDict(ordered_proxy))
            logger.info(f"å·²æ·»åŠ  {len(manual_proxies)} ä¸ªæ‰‹åŠ¨ç²¾é€‰èŠ‚ç‚¹ã€‚")
    except Exception as e:
        logger.warning(f"åŠ è½½æ‰‹åŠ¨èŠ‚ç‚¹æ–‡ä»¶å‡ºé”™: {e}")
    
    return unique_proxies


def get_flag(country_code: str) -> str:
    """å°†å›½å®¶ä»£ç è½¬æ¢ä¸º Emoji å›½æ——"""
    if not country_code or len(country_code) != 2 or country_code == 'UNK':
        return "ğŸ"
    # åŒºåŸŸæŒ‡ç¤ºç¬¦ç¬¦å· A çš„ Unicode æ˜¯ 127462ï¼Œ'A' æ˜¯ 65ï¼Œåç§»é‡ 127397
    return "".join([chr(ord(c) + 127397) for c in country_code.upper()])


def rename_proxies_by_country(proxies: list, db_path: Path, debug: bool = False) -> list:
    """æ ¹æ® IP å½’å±åœ°é‡å‘½åä»£ç†"""
    if not geoip.is_available():
        logger.warning("geoip2 æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡å›½å®¶/åœ°åŒºé‡å‘½åã€‚(è¯·è¿è¡Œ pip install geoip2)")
        return proxies
    
    if not db_path.is_file():
        logger.warning(f"GeoIP æ•°æ®åº“æœªæ‰¾åˆ°: {db_path}ï¼Œè·³è¿‡é‡å‘½åã€‚")
        return proxies

    logger.info("å¼€å§‹æ ¹æ® IP å½’å±åœ°é‡å‘½åèŠ‚ç‚¹...")
    country_counter = {}
    
    for proxy in proxies:
        original_name = proxy.get('name', '')
        # å¦‚æœæ˜¯æ‰‹åŠ¨èŠ‚ç‚¹ï¼ˆåç§°åŒ…å«|M|ï¼‰ï¼Œåˆ™è·³è¿‡é‡å‘½å
        if '|M|' in str(original_name):
            if debug:
                logger.debug(f"è·³è¿‡å¯¹ æ‰‹åŠ¨èŠ‚ç‚¹ çš„é‡å‘½å: {original_name}")
            continue

        server = proxy.get('server')
        if not server:
            continue
            
        code, country, city = geoip.get_ip_country(server, db_path)
        
        # ç»Ÿè®¡è®¡æ•°ï¼Œç”¨äºç”Ÿæˆåºå·
        count = country_counter.get(code, 0) + 1
        country_counter[code] = count
        flag = get_flag(code)
        if code == 'XX':
            new_name = f"{flag} {code} {count:02d}"
        else:
            if city == '':
                new_name = f"{flag} {code}|{country} {count:02d}"
            else:
                new_name = f"{flag} {code}|{country}-{city} {count:02d}"
        proxy['name'] = SingleQuotedString(new_name)
        
        if debug:
            logger.debug(f"Renamed: {original_name} -> {new_name}")
        
    return proxies


def filter_proxies(proxies: list, blocklist_path: Path) -> list:
    """æ‰§è¡Œæ‰€æœ‰è¿‡æ»¤é€»è¾‘ï¼šIPé»‘åå•å’ŒHTTPåè®®è¿‡æ»¤"""
    blocked_ips, blocked_networks = filters.load_ip_blocklist(blocklist_path)

    # IPé»‘åå•è¿‡æ»¤
    if blocked_ips or blocked_networks:
        logger.info("å¼€å§‹æ¸…æ´—ä»£ç† (IPé»‘åå•)")
        original_count = len(proxies)
        proxies = [
            p for p in proxies
            if not filters.is_ip_blocked(p.get('server', ''), blocked_ips, blocked_networks)
        ]
        logger.info(f"æ ¹æ®IPé»‘åå•å…±è¿‡æ»¤äº† {original_count - len(proxies)} ä¸ªä»£ç†ã€‚")

    # HTTPåè®®è¿‡æ»¤
    logger.info("å¼€å§‹è¿‡æ»¤HTTPä»£ç†")
    original_count = len(proxies)
    proxies = [p for p in proxies if p.get('type') != 'http']
    logger.info(f"å…±è¿‡æ»¤äº† {original_count - len(proxies)} ä¸ªHTTPç±»å‹çš„ä»£ç†ã€‚")
    
    return proxies


def save_configs(proxies: list, template_data: dict, output_path: Path):
    """æ„å»ºå¹¶ä¿å­˜æœ€ç»ˆçš„é…ç½®æ–‡ä»¶ (merge.yml å’Œ mobile.yml)"""
    # --- æ–°å¢æ—¶é—´æˆ³èŠ‚ç‚¹ ---
    logger.info("æ–°å¢æ—¶é—´æˆ³èŠ‚ç‚¹")
    update_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    timestamp_node_name = f"[{update_time_str}]-Timestamp"
    timestamp_node = {
        'name': SingleQuotedString(timestamp_node_name),
        'type': 'ss',
        'server': '127.0.0.1',
        'port': 1,
        'cipher': 'none',
        'password': ' '
    }
    # å°†å…¶åŒ…è£…åœ¨ FlowStyleDict ä¸­ä»¥ä¾¿å•è¡Œè¾“å‡º
    proxies.append(FlowStyleDict(timestamp_node))
    logger.info(f"å·²åœ¨ proxies åˆ—è¡¨æœ«å°¾æ–°å¢: {timestamp_node_name}")

    # --- æ„å»ºæœ€ç»ˆé…ç½® ---
    logger.info("å¼€å§‹æ„å»ºæœ€ç»ˆé…ç½®æ–‡ä»¶")
    final_config = template_data
    final_config['proxies'] = proxies

    # --- ä¿å­˜ç»“æœ ---
    save_yaml_file(final_config, output_path)

    # --- ç”Ÿæˆå¹¶ä¿å­˜ mobile.yml ---
    logger.info("æ­£åœ¨ç”Ÿæˆ mobile.yml (å»é™¤ rules å’Œ rule-providers)")
    mobile_config = copy.deepcopy(final_config)
    mobile_config.pop('rules', None)
    mobile_config.pop('rule-providers', None)
    save_yaml_file(mobile_config, config.MOBILE_OUTPUT_FILE)

    logger.info("åˆå¹¶å®Œæˆ")


def main(args):
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    if args.dev:
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.DEBUG)
        for handler in root_logger.handlers:
            handler.setLevel(logging.DEBUG)
        logger.debug("å¼€å‘è€…æ¨¡å¼å·²å¼€å¯ (Log Level: DEBUG)")

    logger.info("å¼€å§‹åˆå¹¶Clashé…ç½®æ–‡ä»¶")

    # --- æ–‡ä»¶è·¯å¾„å®šä¹‰ ---
    sources_path = Path(args.sources).resolve()
    template_path = Path(args.template).resolve()
    blocklist_path = Path(args.blocklist).resolve()
    output_path = Path(args.output).resolve()

    # --- åŠ è½½æ•°æ® ---
    template_data = load_yaml_file(template_path)
    all_proxies, sources_data, has_updates = source_manager.load_and_update_sources(sources_path)
    
    if not has_updates:
        if args.dev:
            logger.info("å¼€å‘è€…æ¨¡å¼ï¼šå¿½ç•¥æ¥æºæ›´æ–°æ£€æµ‹ï¼Œç»§ç»­æ‰§è¡Œã€‚")
        else:
            logger.info("æ‰€æœ‰æ¥æºå‡æ— æ›´æ–°ï¼Œç¨‹åºé€€å‡ºã€‚")
            sys.exit(0)

    # ä¿å­˜æ›´æ–°åçš„ sources.json
    try:
        with sources_path.open('w', encoding='utf-8') as f:
            json.dump(sources_data, f, indent=2, ensure_ascii=False)
        logger.info(f"å·²æ›´æ–°æ¥æºé…ç½®æ–‡ä»¶: {sources_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ¥æºé…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    # --- æ¸…æ´—æ­¥éª¤ ---
    all_proxies = filter_proxies(all_proxies, blocklist_path)

    # --- åˆå¹¶ä¸å»é‡ ---
    logger.info("å¼€å§‹åˆå¹¶ä¸å»é‡")
    unique_proxies = proxy_tools.deduplicate_proxies(all_proxies, debug=args.dev)
    logger.info(f"åˆå¹¶å»é‡åï¼Œæ€»è®¡ {len(unique_proxies)} ä¸ªç‹¬ç«‹ä»£ç†ã€‚")

    # --- æ’åº ---
    logger.info("å¼€å§‹å¯¹ä»£ç†åˆ—è¡¨æŒ‰åç§°æ’åº")
    # ä½¿ç”¨ proxy.get('name', '') ç¡®ä¿å³ä½¿ç¼ºå°‘nameé”®ä¹Ÿä¸ä¼šå‡ºé”™
    unique_proxies.sort(key=lambda p: p.get('name', ''))
    logger.info("æ’åºå®Œæˆã€‚")

    # --- æ£€æŸ¥ä¸æ—§æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ– ---
    if not args.skipcheck:
        if not check_content_changes(unique_proxies, output_path, config.MANUAL_NODES_FILE):
            logger.info("æ£€æµ‹åˆ°è‡ªåŠ¨æŠ“å–èŠ‚ç‚¹æ— å˜åŒ–ï¼Œæ“ä½œæå‰ç»“æŸã€‚")
            sys.exit(0)

    # --- æ›´æ–°èŠ‚ç‚¹æœåŠ¡å™¨ç»Ÿè®¡ ---
    logger.info("å¼€å§‹æ›´æ–°èŠ‚ç‚¹æœåŠ¡å™¨ç»Ÿè®¡")
    # æå–æ‰€æœ‰æœ‰æ•ˆèŠ‚ç‚¹çš„ server å­—æ®µ
    server_ips = [p.get('server') for p in unique_proxies if p.get('server')]
    stats = csvtool.read_stats(config.NODE_STATS_FILE)
    if not args.skipcount:
        csvtool.update_stats(stats, server_ips)
        csvtool.write_stats(config.NODE_STATS_FILE, stats)
        logger.info("èŠ‚ç‚¹æœåŠ¡å™¨ç»Ÿè®¡æ›´æ–°å®Œæˆã€‚")
    else:
        logger.info("èŠ‚ç‚¹æœåŠ¡å™¨ç»Ÿè®¡è·³è¿‡ã€‚")

    # --- æ·»åŠ æ‰‹åŠ¨é…ç½®èŠ‚ç‚¹ ---
    unique_proxies = merge_manual_nodes(unique_proxies, config.MANUAL_NODES_FILE)

    # --- æ ¹æ® IP å½’å±åœ°é‡å‘½å ---
    unique_proxies = rename_proxies_by_country(unique_proxies, config.GEOIP_CITY_DB_FILE, debug=args.dev)
    
    # --- ç»Ÿä¸€æ ¹æ®ç»Ÿè®¡æ•°æ®æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹åç§° ---
    logger.info("æ ¹æ®ç»Ÿè®¡æ•°æ®æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹åç§°")
    unique_proxies = proxy_tools.apply_node_statistics(unique_proxies, stats)

    # --- ä¿å­˜é…ç½®æ–‡ä»¶ ---
    save_configs(unique_proxies, template_data, output_path)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='åˆå¹¶Clashé…ç½®æ–‡ä»¶å¹¶æ ¹æ®é»‘åå•è¿›è¡Œè¿‡æ»¤ã€‚')
    parser.add_argument('--sources', type=str,
                        default=str(config.SOURCES_CONFIG_FILE),
                        help='åŒ…å«ä»£ç†æ¥æº(æœ¬åœ°æ–‡ä»¶æˆ–URL)çš„JSONæ–‡ä»¶è·¯å¾„ã€‚')
    parser.add_argument('--template', type=str,
                        default=str(config.MERGE_TEMPLATE_FILE),
                        help='ç”¨ä½œæœ€ç»ˆé…ç½®ç»“æ„çš„åŸºç¡€æ¨¡æ¿æ–‡ä»¶è·¯å¾„ã€‚')
    parser.add_argument('--blocklist', type=str,
                        default=str(config.IP_BLOCKLIST_FILE),
                        help='IPé»‘åå•æ–‡ä»¶è·¯å¾„ã€‚')
    parser.add_argument('--output', type=str,
                        default=str(config.MERGE_OUTPUT_FILE),
                        help='æœ€ç»ˆåˆå¹¶é…ç½®çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚')
    parser.add_argument('--dev', action='store_true',
                        help='å¯ç”¨å¼€å‘è€…æ¨¡å¼ï¼Œæ‰“å°æ›´è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼ˆä¾‹å¦‚é‡å¤çš„ä»£ç†ï¼‰ã€‚')
    parser.add_argument('--skipcheck', action='store_true',
                        help='è·³è¿‡æ—§æ–‡ä»¶å¯¹æ¯”æ£€æŸ¥')
    parser.add_argument('--skipcount', action='store_true',
                        help='updateæœåŠ¡å™¨è®¡æ•°')

    parsed_args = parser.parse_args()
    main(parsed_args)